#!/usr/bin/env python3
"""
Pipeline Orchestrator Task for Paperboy Backend
Coordinates the full article processing pipeline with error handling and status updates
"""

import uuid
import logging
import json
import time
from datetime import datetime
from typing import Dict, Any, Optional
from celery import Celery, chain, group
import redis

# Configure logging
logger = logging.getLogger(__name__)

# Initialize Celery app
celery_app = Celery('straitwatch')
celery_app.config_from_object('config.celery_config')

# Redis client for pub/sub (optional)
try:
    redis_client = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)
    REDIS_AVAILABLE = True
except Exception as e:
    logger.warning(f"Redis not available for pub/sub: {e}")
    redis_client = None
    REDIS_AVAILABLE = False

@celery_app.task(bind=True, name='run_pipeline_orchestrator')
def run_pipeline_orchestrator(self, article_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Orchestrates the full article processing pipeline using async task chaining
    
    Args:
        article_data: Dictionary containing article data with fields:
            - title: Article title
            - body: Article content
            - id: Optional article ID (will generate if not provided)
            
    Returns:
        Dict containing pipeline execution results and status
    """
    pipeline_id = str(uuid.uuid4())
    start_time = time.time()
    
    # Initialize result structure
    result = {
        "pipeline_id": pipeline_id,
        "status": "success",
        "results": {},
        "errors": {},
        "timestamps": {
            "start": datetime.utcnow().isoformat(),
            "end": None,
            "duration_seconds": None
        }
    }
    
    # Ensure article has an ID
    if 'id' not in article_data or not article_data['id']:
        article_data['id'] = str(uuid.uuid4())
    
    logger.info(f"üöÄ Starting pipeline {pipeline_id} for article: {article_data.get('title', 'Untitled')[:50]}...")
    _emit_status_update(pipeline_id, "started", f"Pipeline started for article: {article_data.get('title', 'Untitled')[:50]}...")
    
    try:
        # Step 1: Scraping (if needed)
        logger.info(f"üì° Step 1: Scraping articles from sources")
        _emit_status_update(pipeline_id, "scraping", "Scraping articles from sources...")
        
        try:
            # Import and run scraping task
            from workers.scraper import run_continuous_scraper
            scraping_result = run_continuous_scraper.delay()
            
            # Don't wait for result - just queue the task
            result["results"]["scraping"] = {"status": "queued", "task_id": scraping_result.id}
            logger.info(f"‚úÖ Scraping task queued: {scraping_result.id}")
            _emit_status_update(pipeline_id, "scraping_queued", f"Scraping task queued: {scraping_result.id}")
            
        except Exception as e:
            error_msg = f"Scraping failed: {str(e)}"
            logger.error(f"‚ùå {error_msg}")
            result["errors"]["scraping"] = error_msg
            result["results"]["scraping"] = "failed"
            _emit_status_update(pipeline_id, "scraping_error", error_msg)
        
        # Step 2: Tagging
        logger.info(f"üè∑Ô∏è Step 2: Tagging articles with NER")
        _emit_status_update(pipeline_id, "tagging", "Tagging articles with NER...")
        
        try:
            # Import and run tagging task
            from workers.pipeline import tag_article_ner
            
            # Get articles from Redis queue for tagging
            from db.redis_queue import get_redis_client
            redis_queue = get_redis_client()
            
            articles_to_tag = []
            while redis_queue.llen('preprocess') > 0:
                article_json = redis_queue.rpop('preprocess')
                if article_json:
                    article = json.loads(article_json)
                    articles_to_tag.append(article)
            
            # Queue tagging tasks without waiting
            tagging_tasks = []
            for article in articles_to_tag:
                tagging_task = tag_article_ner.delay(article)
                tagging_tasks.append(tagging_task.id)
            
            result["results"]["tagging"] = {
                "articles_queued": len(tagging_tasks),
                "task_ids": tagging_tasks
            }
            
            logger.info(f"‚úÖ Tagging queued: {len(tagging_tasks)} articles")
            _emit_status_update(pipeline_id, "tagging_queued", f"Tagging queued: {len(tagging_tasks)} articles")
            
        except Exception as e:
            error_msg = f"Tagging failed: {str(e)}"
            logger.error(f"‚ùå {error_msg}")
            result["errors"]["tagging"] = error_msg
            result["results"]["tagging"] = "failed"
            _emit_status_update(pipeline_id, "tagging_error", error_msg)
        
        # Step 3: Embedding
        logger.info(f"üî¢ Step 3: Embedding articles")
        _emit_status_update(pipeline_id, "embedding", "Embedding articles...")
        
        try:
            # Import and run embedding task
            from workers.pipeline import embed_and_cluster_article
            
            # Since we don't have tagging results, we'll skip embedding for now
            # In a real implementation, we'd need to coordinate between tasks
            embedding_results = []
            logger.info("‚ö†Ô∏è Skipping embedding step - would need task coordination")
            
            result["results"]["embedding"] = {
                "articles_processed": len(embedding_results),
                "successful": len([r for r in embedding_results if r.get('status') == 'success']),
                "failed": len([r for r in embedding_results if r.get('status') != 'success'])
            }
            
            logger.info(f"‚úÖ Embedding completed: {len(embedding_results)} articles processed")
            _emit_status_update(pipeline_id, "embedding_complete", f"Embedding completed: {len(embedding_results)} articles")
            
        except Exception as e:
            error_msg = f"Embedding failed: {str(e)}"
            logger.error(f"‚ùå {error_msg}")
            result["errors"]["embedding"] = error_msg
            result["results"]["embedding"] = "failed"
            _emit_status_update(pipeline_id, "embedding_error", error_msg)
        
        # Step 4: Store to Supabase
        logger.info(f"üíæ Step 4: Storing articles to Supabase")
        _emit_status_update(pipeline_id, "storing_supabase", "Storing articles to Supabase...")
        
        try:
            # Import and run Supabase storage task
            from workers.pipeline import store_to_supabase
            
            # Skip Supabase storage for now since we don't have embedded articles
            supabase_results = []
            logger.info("‚ö†Ô∏è Skipping Supabase storage - no embedded articles")
            
            successful_stores = len([r for r in supabase_results if r.get('status') == 'success'])
            result["results"]["supabase"] = {
                "articles_stored": successful_stores,
                "total_attempted": len(supabase_results)
            }
            
            logger.info(f"‚úÖ Supabase storage completed: {successful_stores} articles stored")
            _emit_status_update(pipeline_id, "supabase_complete", f"Supabase storage completed: {successful_stores} articles")
            
        except Exception as e:
            error_msg = f"Supabase storage failed: {str(e)}"
            logger.error(f"‚ùå {error_msg}")
            result["errors"]["supabase"] = error_msg
            result["results"]["supabase"] = "failed"
            _emit_status_update(pipeline_id, "supabase_error", error_msg)
        
        # Step 5: Store to Faiss
        logger.info(f"üîç Step 5: Storing embeddings to Faiss")
        _emit_status_update(pipeline_id, "storing_faiss", "Storing embeddings to Faiss...")
        
        try:
            # Import and run Faiss storage task
            from workers.pipeline import store_to_faiss
            
            # Skip Faiss storage for now since we don't have embedded articles
            faiss_results = []
            logger.info("‚ö†Ô∏è Skipping Faiss storage - no embedded articles")
            
            successful_stores = len([r for r in faiss_results if r.get('status') == 'success'])
            result["results"]["faiss"] = {
                "embeddings_stored": successful_stores,
                "total_attempted": len(faiss_results)
            }
            
            logger.info(f"‚úÖ Faiss storage completed: {successful_stores} embeddings stored")
            _emit_status_update(pipeline_id, "faiss_complete", f"Faiss storage completed: {successful_stores} embeddings")
            
        except Exception as e:
            error_msg = f"Faiss storage failed: {str(e)}"
            logger.error(f"‚ùå {error_msg}")
            result["errors"]["faiss"] = error_msg
            result["results"]["faiss"] = "failed"
            _emit_status_update(pipeline_id, "faiss_error", error_msg)
        
        # Step 6: Clustering
        logger.info(f"üéØ Step 6: Running clustering")
        _emit_status_update(pipeline_id, "clustering", "Running clustering...")
        
        try:
            # Import and run clustering task
            from workers.cluster import run_clustering
            
            # Queue clustering task without waiting
            clustering_result = run_clustering.delay()
            
            result["results"]["clustering"] = {
                "status": "queued",
                "task_id": clustering_result.id
            }
            
            logger.info(f"‚úÖ Clustering task queued: {clustering_result.id}")
            _emit_status_update(pipeline_id, "clustering_queued", f"Clustering task queued: {clustering_result.id}")
            
        except Exception as e:
            error_msg = f"Clustering failed: {str(e)}"
            logger.error(f"‚ùå {error_msg}")
            result["errors"]["clustering"] = error_msg
            result["results"]["clustering"] = "failed"
            _emit_status_update(pipeline_id, "clustering_error", error_msg)
        
        # Calculate final metrics
        end_time = time.time()
        duration = end_time - start_time
        
        # Determine overall status
        has_errors = bool(result["errors"])
        if has_errors:
            result["status"] = "partial_failure"
        else:
            result["status"] = "success"
        
        result["timestamps"]["end"] = datetime.utcnow().isoformat()
        result["timestamps"]["duration_seconds"] = duration
        
        logger.info(f"üéâ Pipeline {pipeline_id} completed in {duration:.2f}s with status: {result['status']}")
        _emit_status_update(pipeline_id, "completed", f"Pipeline completed in {duration:.2f}s with status: {result['status']}")
        
        return result
        
    except Exception as e:
        error_msg = f"Pipeline orchestration failed: {str(e)}"
        logger.error(f"‚ùå {error_msg}")
        
        end_time = time.time()
        duration = end_time - start_time
        
        result["status"] = "failed"
        result["errors"]["orchestration"] = error_msg
        result["timestamps"]["end"] = datetime.utcnow().isoformat()
        result["timestamps"]["duration_seconds"] = duration
        
        _emit_status_update(pipeline_id, "failed", error_msg)
        
        return result

def _emit_status_update(pipeline_id: str, stage: str, message: str) -> None:
    """Emit status update to Redis pub/sub if available"""
    if REDIS_AVAILABLE and redis_client:
        try:
            status_data = {
                "pipeline_id": pipeline_id,
                "stage": stage,
                "message": message,
                "timestamp": datetime.utcnow().isoformat()
            }
            redis_client.publish(f"pipeline_status_{pipeline_id}", json.dumps(status_data))
        except Exception as e:
            logger.warning(f"Failed to emit status update: {e}")

@celery_app.task(bind=True, name='run_async_pipeline_chain')
def run_async_pipeline_chain(self, sources=None, max_articles_per_source=10) -> Dict[str, Any]:
    """
    Run pipeline using Celery's async task chaining
    
    Args:
        sources: List of source dictionaries
        max_articles_per_source: Maximum articles per source
        
    Returns:
        dict: Pipeline chain result
    """
    try:
        logger.info("üîÑ Starting async pipeline chain")
        
        # Create task chain: Scraping -> Tagging -> Embedding -> Clustering
        from workers.scraper import run_continuous_scraper
        from workers.pipeline import tag_article_ner
        from workers.pipeline import embed_and_cluster_article
        from workers.cluster import run_clustering
        
        # Create the chain
        pipeline_chain = chain(
            run_continuous_scraper.s(sources, max_articles_per_source),
            tag_article_ner.s(),
            embed_and_cluster_article.s(),
            run_clustering.s()
        )
        
        # Execute the chain asynchronously
        chain_result = pipeline_chain.delay()
        
        logger.info(f"‚úÖ Async pipeline chain queued: {chain_result.id}")
        
        return {
            "status": "chain_queued",
            "chain_id": chain_result.id,
            "message": "Pipeline chain queued successfully"
        }
        
    except Exception as e:
        error_msg = f"Async pipeline chain failed: {str(e)}"
        logger.error(f"‚ùå {error_msg}")
        return {
            "status": "failed",
            "error": error_msg
        }

@celery_app.task(bind=True, name='run_parallel_pipeline_steps')
def run_parallel_pipeline_steps(self, articles: list) -> Dict[str, Any]:
    """
    Run pipeline steps in parallel using Celery's group
    
    Args:
        articles: List of articles to process
        
    Returns:
        dict: Parallel processing results
    """
    try:
        logger.info(f"üîÑ Starting parallel pipeline processing for {len(articles)} articles")
        
        from workers.pipeline import tag_article_ner
        from workers.pipeline import embed_and_cluster_article
        
        # Create parallel tasks for tagging
        tagging_tasks = group([
            tag_article_ner.s(article) for article in articles
        ])
        
        # Execute tagging in parallel
        tagging_result = tagging_tasks.delay()
        
        logger.info(f"‚úÖ Parallel tagging queued: {tagging_result.id}")
        
        return {
            "status": "parallel_queued",
            "tagging_group_id": tagging_result.id,
            "articles_count": len(articles)
        }
        
    except Exception as e:
        error_msg = f"Parallel pipeline steps failed: {str(e)}"
        logger.error(f"‚ùå {error_msg}")
        return {
            "status": "failed",
            "error": error_msg
        }

# Export the task
__all__ = ['run_pipeline_orchestrator'] 